{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jortegon/materialsGAN/blob/main/materialsGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnYaCcsqelI3"
      },
      "outputs": [],
      "source": [
        "!lscpu |grep 'Model name'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sección nueva"
      ],
      "metadata": {
        "id": "_ONTHBVz7Jjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxATPdNfYFnv"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIy7XUY-69OU"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbQnbPsk98en"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viYZAKuMa4fD"
      },
      "outputs": [],
      "source": [
        "# Debug mode (on/off)\n",
        "%pdb off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25CQ_P3xh5HR"
      },
      "outputs": [],
      "source": [
        "# Main imports\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaBJ1LMSKDq6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "def download_file(path: str):\n",
        "    \"\"\"\n",
        "    Download file from content/\n",
        "    \"\"\"\n",
        "    files.download(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeZMVaEgWOCp"
      },
      "outputs": [],
      "source": [
        "from shutil import make_archive, unpack_archive\n",
        "\n",
        "\n",
        "def zip_files(files: List[str]):\n",
        "    \"\"\"\n",
        "    zip and download files\n",
        "    \"\"\"\n",
        "    for f in files:\n",
        "        make_archive(\n",
        "            f'/content/{f}',\n",
        "            'zip',\n",
        "            f'/content/',\n",
        "            f'{f}',\n",
        "        )\n",
        "        download_file(f'{f}.zip')\n",
        "\n",
        "\n",
        "def unzip_files(files: List[str]):\n",
        "    for f in files:\n",
        "        unpack_archive(\n",
        "            f'/content/{f}.zip',\n",
        "            '/content/',\n",
        "            'zip',\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHNSwRPJ-ABN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "def my_plot_model(model):\n",
        "    \"\"\"\n",
        "    Plot and save model design\n",
        "    \"\"\"\n",
        "    name = f'{model.name}.png'\n",
        "    plot_model(\n",
        "        model,\n",
        "        to_file=name,\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True,\n",
        "        expand_nested=True,\n",
        "    )\n",
        "    download_file(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSXfhZchnOxt"
      },
      "outputs": [],
      "source": [
        "from matplotlib.image import imsave\n",
        "\n",
        "\n",
        "def save_img(source, name: str, dir: str):\n",
        "    \"\"\"\n",
        "    Saves images from source in specific dir\n",
        "    \"\"\"\n",
        "    img = source[0, :, :, 0]\n",
        "    path = f'{dir}/img_{name}.png'\n",
        "    imsave(\n",
        "        path,\n",
        "        img,\n",
        "        # dpi=params['IMG_SIZE'],\n",
        "        cmap='gray'\n",
        "    )\n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3b8C2pxGSNY"
      },
      "outputs": [],
      "source": [
        "def show_img(source):\n",
        "    \"\"\"\n",
        "    Display image from source\n",
        "    \"\"\"\n",
        "    display.clear_output(wait=True)\n",
        "    img = source[0, :, :, 0]\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    time.sleep(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osR_rmsWQyDb"
      },
      "outputs": [],
      "source": [
        "def timed(func):\n",
        "    \"\"\"\n",
        "    Prints elapsed time for function\n",
        "    \"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        before = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        after = time.time()\n",
        "        fname = func.__name__\n",
        "        print(f'{fname}: {(after - before)} secs')\n",
        "        return result\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkWK8AP_XEN3"
      },
      "outputs": [],
      "source": [
        "@timed\n",
        "def fit_model(model, kwargs):\n",
        "    return model.fit(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb38hOYQ5vVK"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuT66h6uLzgb"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2rGIeYx6JuF"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKsPW-dgp771"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "def set_tensorboard_callback():\n",
        "    \"\"\"\n",
        "    TensorBoard Callback\n",
        "    \"\"\"\n",
        "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    board = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    return board\n",
        "\n",
        "\n",
        "tensorboard_callback = set_tensorboard_callback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX81eImT6Pmy"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "os.system(f'rm -rf ./logs/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RAbQPWEMAKQ"
      },
      "source": [
        "### Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN1Iud_evppC"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QErEmEgTN5iG"
      },
      "source": [
        "# Source DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pddSaBAShTJE"
      },
      "source": [
        "## Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb8YrPe_haXZ"
      },
      "outputs": [],
      "source": [
        "# Import data from Drive\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive_dir = '/content/drive'\n",
        "drive.mount(drive_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxwqU-iChdoK"
      },
      "outputs": [],
      "source": [
        "# Load SEM images\n",
        "def get_path(drive_dir: str):\n",
        "    \"\"\"\n",
        "    Drive for original_data/\n",
        "    11 classes\n",
        "    700, 750, 800,\n",
        "    N700, N750, N800,\n",
        "    SKPH, SKPHD,\n",
        "    SPY - Sargazo pirolizado\n",
        "    K750, KSPY,\n",
        "    \"\"\"\n",
        "    data_path = drive_dir + '/My Drive/New_Tesis/data/original_data/'\n",
        "    class_path = '700/'\n",
        "\n",
        "    # Dataset\n",
        "    DATASET_PATH = data_path + class_path\n",
        "\n",
        "    # Classes = ['M0', 'M1', 'M2', 'M3']\n",
        "    CLASSES = [\n",
        "        d for d in os.listdir(DATASET_PATH) if os.path.isdir(\n",
        "            os.path.join(DATASET_PATH, d)\n",
        "        )\n",
        "    ]\n",
        "    return DATASET_PATH, CLASSES\n",
        "\n",
        "\n",
        "DATASET_PATH, CLASSES = get_path(drive_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0731yIxBi3n"
      },
      "source": [
        "## Init PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owEnRRWYb2yx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def set_params(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Set global params as dict\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "    params['DATASET_PATH'] = path\n",
        "\n",
        "    img_size = 320          # @param {type:\"integer\"}\n",
        "    params['IMG_SIZE'] = 32 * (img_size // 32)\n",
        "\n",
        "    batch_size = 32         # @param {type:\"integer\"}\n",
        "    params['BATCH_SIZE'] = batch_size\n",
        "\n",
        "    # max colab 64\n",
        "    latent_dim = 64         # @param {type:\"integer\"}\n",
        "    params['LATENT_DIM'] = latent_dim\n",
        "\n",
        "    channels = 1\n",
        "    img_size = params.get('IMG_SIZE')\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        params['INPUT_SHAPE'] = (\n",
        "            channels, img_size, img_size\n",
        "        )\n",
        "    else:\n",
        "        params['INPUT_SHAPE'] = (\n",
        "            img_size, img_size, channels\n",
        "        )\n",
        "    return params\n",
        "\n",
        "\n",
        "PARAMS = set_params(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N20p9ravKPak"
      },
      "outputs": [],
      "source": [
        "@timed\n",
        "def init_dataframe(path: str) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Initialize dataframe from source images path\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'id': [],\n",
        "        'path': [],\n",
        "        'class': [],\n",
        "        'lbl': [],\n",
        "    }\n",
        "    for cls in CLASSES:\n",
        "        cls_path = os.path.join(path, cls)\n",
        "        for f in os.listdir(cls_path):\n",
        "            if f.endswith('.png'):\n",
        "                data['id'].append(f)\n",
        "                data['path'].append(os.path.join(cls_path, f))\n",
        "                data['class'].append(cls)\n",
        "                data['lbl'].append(None)\n",
        "\n",
        "    return DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax6FQcdDrLT5"
      },
      "outputs": [],
      "source": [
        "MAIN_DF = init_dataframe(DATASET_PATH)\n",
        "MAIN_DF.info()\n",
        "MAIN_DF.describe()\n",
        "MAIN_DF.to_csv('main_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGZvBZKIK8qA"
      },
      "source": [
        "# Manual Select"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY7jmNnYe-eZ"
      },
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owroaqr9eAST"
      },
      "outputs": [],
      "source": [
        "# CSC Generator\n",
        "DATAGEN = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.25\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcodwAdYWSg4"
      },
      "outputs": [],
      "source": [
        "# Ask\n",
        "def ask(img, name: str, i=0) -> str:\n",
        "    \"\"\"\n",
        "    Shows image and returns user input\n",
        "    \"\"\"\n",
        "    show_img(img)\n",
        "    ans = str(input(f\"{i}:{name} works (w) or nah?\\n\")).lower()\n",
        "    return ans\n",
        "\n",
        "\n",
        "# Ratio msgs\n",
        "def manual_lim(lbl_counts: List[int], lim=0.95) -> bool:\n",
        "    \"\"\"\n",
        "    Selection loop control\n",
        "    \"\"\"\n",
        "    result = lbl_counts[1] / lbl_counts[0]\n",
        "    msg = f'{lbl_counts[1]}/{lbl_counts[0]} {result:.2f}: '\n",
        "    if result > lim:\n",
        "        display.clear_output(wait=True)\n",
        "        print(msg + 'finishing manual dataframe!')\n",
        "        return True\n",
        "    print(msg + 'not there yet!!')\n",
        "    return False\n",
        "\n",
        "\n",
        "# Manual selections\n",
        "@timed\n",
        "def manual_select(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Manual selection loop\n",
        "    \"\"\"\n",
        "    manual_generator = DATAGEN.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=DATASET_PATH,\n",
        "        x_col=\"path\", y_col=\"id\",\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        class_mode='raw',\n",
        "        target_size=(\n",
        "            PARAMS.get('IMG_SIZE'), PARAMS.get('IMG_SIZE'),\n",
        "        ),\n",
        "    )\n",
        "    # manual lims\n",
        "    #lim = df.count()[0]\n",
        "    lim = len(manual_generator)\n",
        "    df_min = int(lim * 0.1)\n",
        "    df_min = df_min if df_min > 100 else 100\n",
        "    df_min = df_min if df_min < lim else lim\n",
        "\n",
        "    df_max = int(lim * 0.2)\n",
        "    df_max = df_max if df_max > 100 else 100\n",
        "    df_max = df_max if df_max < lim else lim\n",
        "\n",
        "    # manual loop\n",
        "    pre_img = [pd.NA, pd.NA]\n",
        "    for i in range(len(manual_generator)):\n",
        "        # next\n",
        "        img, id = next(manual_generator)\n",
        "        id = id[0]\n",
        "\n",
        "        # show and ask\n",
        "        response = ask(img, id, i)\n",
        "        if response == 'exit':\n",
        "            break\n",
        "        if i > 0 and response == 'back':\n",
        "            response = ask(*pre_img, i - 1)\n",
        "\n",
        "        img_n = df[df['id'] == id].index.item()\n",
        "        df['lbl'][img_n] = 'YES' if response == 'w' else 'NO'\n",
        "\n",
        "        # limit control\n",
        "        if i < df_min:\n",
        "            print(f'{i}/{df_min}: not enough data!!')\n",
        "        elif i > df_max:\n",
        "            print(f'{i}: finishing, too much data!')\n",
        "            break\n",
        "        # ratio\n",
        "        elif manual_lim(df.value_counts(subset=['lbl'])):\n",
        "            break\n",
        "\n",
        "        # wait result\n",
        "        time.sleep(0.2)\n",
        "        pre_img = [img, id]\n",
        "\n",
        "    # copy and clean df\n",
        "    manual_df = df.copy()\n",
        "    manual_df.dropna(inplace=True, subset=['lbl'])\n",
        "    manual_df.info()\n",
        "\n",
        "    return manual_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xV-a5jlKhNo"
      },
      "source": [
        "## Select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zoCwoy65PKY"
      },
      "outputs": [],
      "source": [
        "manual_df = manual_select(MAIN_DF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftQeby8EK1r_"
      },
      "outputs": [],
      "source": [
        "manual_df.to_csv('manual_df.csv', index=False)\n",
        "download_file('manual_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lomIbvTnvhz"
      },
      "outputs": [],
      "source": [
        "manual_df = pd.read_csv('manual_df.csv')\n",
        "manual_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFC28MGNYFVP"
      },
      "source": [
        "# CSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVhui3nEa9Ch"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho1HSctIiWUI"
      },
      "outputs": [],
      "source": [
        "# Classifier (API mode)\n",
        "\n",
        "\n",
        "@timed\n",
        "def build_classifier(input_shape: List[int]):\n",
        "    \"\"\"\n",
        "    Returns binary classifier model\n",
        "    \"\"\"\n",
        "    # input\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # conv1\n",
        "    conv_1 = layers.Conv2D(8, 8, activation='relu')(inputs)\n",
        "    mpool_1 = layers.MaxPool2D(2)(conv_1)\n",
        "\n",
        "    # conv2\n",
        "    conv_2 = layers.Conv2D(16, 8, activation='relu')(mpool_1)\n",
        "    mpool_2 = layers.MaxPool2D(2)(conv_2)\n",
        "\n",
        "    # dense\n",
        "    flat = layers.Flatten()(mpool_2)\n",
        "    densebig = layers.Dense(32, activation='relu')(flat)\n",
        "    dropbig = layers.Dropout(0.2)(densebig)\n",
        "\n",
        "    # output\n",
        "    dense = layers.Dense(1, activation='sigmoid')(dropbig)\n",
        "\n",
        "    # model\n",
        "    model = tf.keras.Model(\n",
        "        inputs=inputs,\n",
        "        outputs=dense,\n",
        "        name='classifier'\n",
        "    )\n",
        "\n",
        "    # compile\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qnk5AqYqqBg"
      },
      "outputs": [],
      "source": [
        "# Summary\n",
        "csc = build_classifier(PARAMS.get('INPUT_SHAPE'))\n",
        "my_plot_model(csc)\n",
        "csc.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxHOYl_gqUr-"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1aPl8xf9G49"
      },
      "outputs": [],
      "source": [
        "# Open\n",
        "class_df = pd.read_csv('manual_df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYcJq2vduGfb"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PTrR3Bq9u6q"
      },
      "outputs": [],
      "source": [
        "class myEarlyCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Stops train loop when thresholds met\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience: int):\n",
        "        super().__init__()\n",
        "        self.patience = patience\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # The number of epoch it has waited when loss is no longer minimum.\n",
        "        self.wait = 0\n",
        "        # The epoch the training stops at.\n",
        "        self.stopped_epoch = 0\n",
        "        # Initialize best\n",
        "        self.best_acc = 0\n",
        "        self.best_val_acc = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # Get logs info\n",
        "        acc = logs.get('acc')\n",
        "        val_acc = logs.get('val_acc')\n",
        "\n",
        "        # Greater\n",
        "        is_acc = np.greater(acc, self.best_acc)\n",
        "        is_val = np.greater(val_acc, self.best_val_acc)\n",
        "        if is_acc or is_val:\n",
        "            if is_acc:\n",
        "                self.best_acc = acc\n",
        "            if is_val:\n",
        "                self.best_val_acc = val_acc\n",
        "                self.best_weights = self.model.get_weights()\n",
        "            self.wait -= 1\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            # Patience stop\n",
        "            if self.wait > self.patience:\n",
        "                print(\n",
        "                    f'\\nPatience met - acc:{acc:.2f}, val_acc:{val_acc:.2f}!'\n",
        "                )\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1dHv6l6PB_S"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def csc_callbacks() -> List:\n",
        "    \"\"\"\n",
        "    Set classifier training callbacks\n",
        "    \"\"\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        f'CSC.tf',\n",
        "        monitor='acc',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return [\n",
        "        checkpoint,\n",
        "        myEarlyCallback(patience=10),\n",
        "        tensorboard_callback\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBXB3MuLhyJL"
      },
      "outputs": [],
      "source": [
        "def csc_kwargs(df: DataFrame, epochs: int):\n",
        "    \"\"\"\n",
        "    Custom classifier kwargs generator\n",
        "    \"\"\"\n",
        "    # Batch size\n",
        "    manual_batch = df.count()[0] // 5\n",
        "\n",
        "    # Train flow\n",
        "    train_flow = DATAGEN.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=DATASET_PATH,\n",
        "        x_col=\"path\", y_col=\"lbl\",\n",
        "        subset=\"training\",\n",
        "        batch_size=manual_batch,\n",
        "        seed=42,\n",
        "        shuffle=True,\n",
        "        class_mode=\"binary\",\n",
        "        color_mode='grayscale',\n",
        "        target_size=(\n",
        "            PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE']\n",
        "        ),\n",
        "    )\n",
        "    step_size_train = train_flow.n // train_flow.batch_size\n",
        "\n",
        "    # Validation flow\n",
        "    valid_flow = DATAGEN.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=DATASET_PATH,\n",
        "        x_col=\"path\", y_col=\"lbl\",\n",
        "        subset=\"validation\",\n",
        "        batch_size=manual_batch // 5,\n",
        "        seed=42,\n",
        "        shuffle=True,\n",
        "        class_mode=\"binary\",\n",
        "        color_mode='grayscale',\n",
        "        target_size=(\n",
        "            PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE']\n",
        "        ),\n",
        "    )\n",
        "    step_size_valid = valid_flow.n // valid_flow.batch_size\n",
        "\n",
        "    return {\n",
        "        'x': train_flow,\n",
        "        'steps_per_epoch': step_size_train,\n",
        "        'validation_data': valid_flow,\n",
        "        'validation_steps': step_size_valid,\n",
        "        'epochs': epochs,\n",
        "        'callbacks': csc_callbacks(),\n",
        "        'verbose': 1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGRYz7LY9KJF"
      },
      "source": [
        "### history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv-3BGbIinzl"
      },
      "outputs": [],
      "source": [
        "eps = 100     # @param {type:\"integer\"}\n",
        "fit_kwargs = csc_kwargs(class_df, epochs=eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7lNB5IPIqH8"
      },
      "outputs": [],
      "source": [
        "history = fit_model(csc, fit_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm4ITR8CJbZ1"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4G5BXh_O64F"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2IHXgxsLSL9"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojenWkZg6yrL"
      },
      "outputs": [],
      "source": [
        "CSC_NAME = 'csc'    # @param {type: 'string'}\n",
        "csc.save(CSC_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEs-ir4_L2Xe"
      },
      "outputs": [],
      "source": [
        "zip_files(['csc', 'CSC.tf'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UokGlFHQOxCP"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBTccwitsuK1"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH9DzkLnFKzh"
      },
      "outputs": [],
      "source": [
        "unzip_files(['csc', 'CSC.tf'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E-6Nz1wGm46"
      },
      "outputs": [],
      "source": [
        "class_df = pd.read_csv('manual_df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mge1fRJ5I4Gc"
      },
      "source": [
        "### Restore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ops2_TNE1Wm"
      },
      "outputs": [],
      "source": [
        "unzip_name = 'csc'     # @param {type: 'string'}\n",
        "\n",
        "saved_csc = tf.keras.models.load_model(\n",
        "    f'/content/{unzip_name}'\n",
        ")\n",
        "saved_csc.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47X3Ry9WGPZd"
      },
      "outputs": [],
      "source": [
        "eps = 20     # @param {type:\"integer\"}\n",
        "fit_kwargs = csc_kwargs(class_df, epochs=eps)\n",
        "history = fit_model(saved_csc, fit_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RfBFC7kPz7H"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suWdXGTsca_c"
      },
      "outputs": [],
      "source": [
        "@timed\n",
        "def predict(model):\n",
        "    \"\"\"\n",
        "    Get classifier model predictions from DATASET_PATH/CLASSES\n",
        "    \"\"\"\n",
        "    predict_flow = DATAGEN.flow_from_dataframe(\n",
        "        dataframe=MAIN_DF,\n",
        "        directory=DATASET_PATH,\n",
        "        x_col='path', y_col='id',\n",
        "        shuffle=False,\n",
        "        class_mode=None,\n",
        "        color_mode='grayscale',\n",
        "        target_size=(\n",
        "            PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE']\n",
        "        ),\n",
        "    )\n",
        "    return model.predict(predict_flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSOOYn9UJi-i"
      },
      "outputs": [],
      "source": [
        "# First prediction [csc, saved_csc]\n",
        "results = predict(saved_csc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uukvHrXWojT0"
      },
      "source": [
        "### Relearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpEdwz5XSZEn"
      },
      "outputs": [],
      "source": [
        "# Relearn DF\n",
        "@timed\n",
        "def relearn(df: DataFrame, results: np.ndarray) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns copy of maindf adding predictions\n",
        "    \"\"\"\n",
        "    #learn_lim = results.mean()\n",
        "    #learn_lim = results.max() - results.min() / 2\n",
        "    learn_lim = results.mean() + results.std() ** 2\n",
        "\n",
        "    df['val'] = [r[0] for r in results.tolist()]\n",
        "\n",
        "    df.loc[(df['lbl'].isna()) & (df['val'] < learn_lim), 'lbl'] = 'NO'\n",
        "    df.loc[(df['lbl'].isna()) & (df['val'] >= learn_lim), 'lbl'] = 'YES'\n",
        "\n",
        "    # save and download\n",
        "    df.to_csv('relearn_df.csv', index=False)\n",
        "    download_file('relearn_df.csv')\n",
        "    print(\n",
        "        f'Saved relearn_df.csv (learn_lim :{learn_lim:.5f})'\n",
        "    )\n",
        "    df.info()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxOGY7htePux"
      },
      "outputs": [],
      "source": [
        "relearn_df = relearn(MAIN_DF.copy(), results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49SsXYEQkHj7"
      },
      "source": [
        "### Learnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlljRPdf4cNI"
      },
      "outputs": [],
      "source": [
        "def csc_results(df: DataFrame, shows: int):\n",
        "    \"\"\"\n",
        "    Show preclassifier results\n",
        "    \"\"\"\n",
        "    learnt_generator = DATAGEN.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=DATASET_PATH,\n",
        "        x_col='path', y_col='lbl',\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        class_mode='raw',\n",
        "        color_mode='grayscale',\n",
        "        target_size=(\n",
        "            PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE']\n",
        "        )\n",
        "    )\n",
        "    true_limit, false_limit = shows, shows\n",
        "    for i in range(len(learnt_generator)):\n",
        "        img, lbl = next(learnt_generator)\n",
        "        if lbl == 'YES':\n",
        "            true_limit -= 1\n",
        "        else:\n",
        "            false_limit -= 1\n",
        "        show_img(img)\n",
        "        answer = input(f'{i} - Label: {lbl}\\nPress any key to continue\\n')\n",
        "        if answer == 'exit' or (true_limit < 0 and false_limit < 0):\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2m8wsQKdX5g"
      },
      "outputs": [],
      "source": [
        "read_relearn_df = pd.read_csv('relearn_df.csv')\n",
        "csc_results(read_relearn_df, shows=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apni7KprjuAC"
      },
      "source": [
        "# C shared lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLzjbYXlXMaf"
      },
      "source": [
        "## C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiZLfgIqwTV_"
      },
      "outputs": [],
      "source": [
        "%%file f2s.c\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "    void *\n",
        "    caracterizacion(char *pix, int width, int height)\n",
        "{\n",
        "    /*\n",
        "    Parte para hacer el conteo de lineas\n",
        "    hace tanto el conteo de inicio y fin\n",
        "    como el conteo de toda la lÌnea\n",
        "    */\n",
        "    unsigned char *pixels = (unsigned char *)pix;\n",
        "    int r, c, ls;\n",
        "    int largo = width > height ? width : height;\n",
        "    int largo2;\n",
        "    int **cont;\n",
        "    int first_pix, negado;\n",
        "    cont = (int **)malloc(5 * sizeof(int *));\n",
        "    /*\n",
        "    Esto se hace para que las funciones de correlación\n",
        "    solo se calcule hasta la mitad, de tal forma que se\n",
        "    mantengan la dependencia lineal entre las funciones de cada fase.\n",
        "    */\n",
        "    largo = largo / 2;\n",
        "\n",
        "    for (r = 0; r < 5; r++)\n",
        "    {\n",
        "        *(cont + r) = (int *)malloc(largo * sizeof(int));\n",
        "        memset((void *)(*(cont + r)), 0, largo * sizeof(int));\n",
        "    }\n",
        "    for (r = 0; r < height; r++)\n",
        "    {\n",
        "        for (c = 0; c < width; c++)\n",
        "        {\n",
        "            first_pix = pixels[r * width + c];\n",
        "            if (first_pix)\n",
        "            {\n",
        "                /* first_pix es 1 */\n",
        "                negado = first_pix;\n",
        "                largo2 = largo < width - c ? largo : width - c;\n",
        "                for (ls = 0; ls < largo2; ls++)\n",
        "                {\n",
        "                    cont[4][ls]++;\n",
        "                    /* cuenta los unos en los extremos */\n",
        "                    cont[0][ls] += (first_pix & pixels[r * width + c + ls]);\n",
        "                    /* Verifica si siguen siendo unos en la linea */\n",
        "                    negado = negado & pixels[r * width + c + ls];\n",
        "                    cont[1][ls] += negado;\n",
        "                }\n",
        "                negado = first_pix;\n",
        "                largo2 = largo < height - r ? largo : height - r;\n",
        "                for (ls = 0; ls < largo2; ls++)\n",
        "                {\n",
        "                    cont[4][ls]++;\n",
        "                    /* cuenta los unos en los extremos */\n",
        "                    cont[0][ls] += (first_pix & pixels[(r + ls) * width + c]);\n",
        "                    /* Verifica si siguen siendo unos en la linea */\n",
        "                    negado = negado & pixels[(r + ls) * width + c];\n",
        "                    cont[1][ls] += negado;\n",
        "                }\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "                /* first_pix es cero */\n",
        "                first_pix = 1;\n",
        "                negado = 1;\n",
        "                largo2 = largo < width - c ? largo : width - c;\n",
        "                for (ls = 0; ls < largo2; ls++)\n",
        "                {\n",
        "                    cont[4][ls]++;\n",
        "                    /* \n",
        "                    cuenta los ceros en los extremos, solo puede variar el \n",
        "\t\t            ultimo bit, por lo que busco que sea diferente a 1\n",
        "                    */\n",
        "                    cont[2][ls] += (first_pix ^ pixels[r * width + c + ls]);\n",
        "                    /* Verifica si siguen siendo ceros en la linea */\n",
        "                    negado = negado & (first_pix ^ pixels[r * width + c + ls]);\n",
        "                    cont[3][ls] += negado;\n",
        "                }\n",
        "                negado = 1;\n",
        "                largo2 = largo < height - r ? largo : height - r;\n",
        "                for (ls = 0; ls < largo2; ls++)\n",
        "                {\n",
        "                    cont[4][ls]++;\n",
        "                    /*\n",
        "                    cuenta los ceros en los extremos, solo puede variar el \n",
        "\t\t            ultimo bit, por lo que busco que sea diferente a 1\n",
        "                    */\n",
        "                    cont[2][ls] += (first_pix ^ pixels[(r + ls) * width + c]);\n",
        "                    /* Verifica si siguen siendo ceros en la linea */\n",
        "                    negado = negado & (first_pix ^ pixels[(r + ls) * width + c]);\n",
        "                    cont[3][ls] += negado;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return cont;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHbN889uwQVD"
      },
      "outputs": [],
      "source": [
        "os.system('gcc -c -Wall -Werror -fpic f2s.c')\n",
        "os.system('gcc -shared -o f2s.so f2s.o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwe-hfCLOBFs"
      },
      "source": [
        "## Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azHr8wugeykz"
      },
      "outputs": [],
      "source": [
        "import ctypes\n",
        "\n",
        "\n",
        "def load_f2s():\n",
        "    \"\"\"\n",
        "    Load the shared library into ctypes\n",
        "    \"\"\"\n",
        "    libname = \"./f2s.so\"\n",
        "    c_lib = ctypes.CDLL(libname)\n",
        "    c_lib.caracterizacion.restype = ctypes.POINTER(\n",
        "        ctypes.POINTER(ctypes.c_int)\n",
        "    )\n",
        "    return c_lib\n",
        "\n",
        "\n",
        "C_LIB = load_f2s()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nOiyr3ckJ8o"
      },
      "outputs": [],
      "source": [
        "def caracterization(img, size: int):\n",
        "    \"\"\"\n",
        "    Returns implemented caracterization results from input img\n",
        "    \"\"\"\n",
        "    # Numpy array of booleans\n",
        "    b_img = img > img.mean()\n",
        "    #b_img = img > (img.max() - img.mean() / 2)\n",
        "    #b_img = img > (img.max() - img.min() / 2)\n",
        "\n",
        "    # Define array of boolean chars\n",
        "    char_array = np.ctypeslib.as_ctypes(b_img)\n",
        "\n",
        "    # Process img\n",
        "    f2s_c_data = C_LIB.caracterizacion(char_array, size, size)\n",
        "\n",
        "    max_index = int(size / 2)\n",
        "    f2s_py = np.zeros((5, max_index))\n",
        "    for ftype in range(5):\n",
        "        for index in range(max_index):\n",
        "            f2s_py[ftype][index] = f2s_c_data[ftype][index]\n",
        "    f2s_py = f2s_py / np.array(f2s_py[4, :])\n",
        "\n",
        "    return f2s_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M552gesrx-69"
      },
      "outputs": [],
      "source": [
        "@timed\n",
        "def caract_df(csv_file: str, path: str) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Fill FF data for input df in path\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "    caract_gen = DATAGEN.flow_from_dataframe(\n",
        "        df,\n",
        "        directory=path,\n",
        "        x_col=\"path\", y_col=\"id\",\n",
        "        class_mode='raw',\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        color_mode='grayscale',\n",
        "        target_size=(\n",
        "            PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE'],\n",
        "        ),\n",
        "    )\n",
        "    results = []\n",
        "    for _ in range(len(caract_gen)):\n",
        "        img, _ = next(caract_gen)\n",
        "        f2s_py = caracterization(img, PARAMS['IMG_SIZE'])\n",
        "        results.append(f2s_py)\n",
        "\n",
        "    df['ff'] = results\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdmmn8xSwFwu"
      },
      "source": [
        "## Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_kEFHE-eBXH"
      },
      "outputs": [],
      "source": [
        "def plot_avg(results: List):\n",
        "    fig = plt.figure(constrained_layout=True)\n",
        "\n",
        "    ax = fig.add_subplot()\n",
        "    ax.plot(results[:4, :].T)\n",
        "    ax.set_xlabel(\"Distance\")\n",
        "    ax.set_ylabel(\"Value\")\n",
        "    ax.set_title(\"Avg Correlations\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_separate(r: List):\n",
        "    fig, (ax1, ax2) = plt.subplots(\n",
        "        1, 2,\n",
        "        sharex=True, sharey=True,\n",
        "        constrained_layout=True\n",
        "    )\n",
        "    \n",
        "    ax1.set_xlabel(\"Distance\")\n",
        "    ax1.set_ylabel(\"Value\")\n",
        "    ax1.set_title('1s')\n",
        "    ax1.plot(r[0, :].T, 'tab:blue', label='F2P')\n",
        "    ax1.plot(r[1, :].T, 'tab:orange', label='FLP')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_xlabel(\"Distance\")\n",
        "    ax2.set_ylabel(\"Value\")\n",
        "    ax2.set_title('0s')\n",
        "    ax2.plot(r[2, :].T, 'tab:green', label='F2P')\n",
        "    ax2.plot(r[3, :].T, 'tab:red', label='FLP')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "@timed\n",
        "def plots(df: DataFrame, lbl: str):\n",
        "    \"\"\"\n",
        "    Plots results\n",
        "    \"\"\"\n",
        "    r = df['ff'][df['lbl'] == lbl].mean()\n",
        "    \n",
        "    plot_avg(r)\n",
        "    plot_separate(r)\n",
        "   \n",
        "    #plt.savefig(f'{path}/plot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eDGmXlc49ea"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('results_df.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbJaHP0b6Mq4"
      },
      "outputs": [],
      "source": [
        "r = df['ff'][df['lbl'] == \"YES\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RjcKVaePfAL"
      },
      "outputs": [],
      "source": [
        "df['ff'][df['lbl'] == \"YES\"].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyYAV0KjbgOr"
      },
      "source": [
        "### FFS (Relearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC7dFGHjOvZ6"
      },
      "outputs": [],
      "source": [
        "results_df = caract_df(\n",
        "    'relearn_df.csv',\n",
        "    DATASET_PATH,\n",
        ")\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j37uBKb2zBUu"
      },
      "outputs": [],
      "source": [
        "plots(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5e1u10VyJf4"
      },
      "outputs": [],
      "source": [
        "plots(results_df, 'NO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXAXRagiabC5"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv('results_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0CHTQtVw-Ev"
      },
      "source": [
        "### Test (Manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3cgafTeJZyh"
      },
      "outputs": [],
      "source": [
        "test_df = caract_df('manual_df.csv', DATASET_PATH)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXWSUogy1igy"
      },
      "outputs": [],
      "source": [
        "plots(test_df, 'YES')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waJDoIdf1kbc"
      },
      "outputs": [],
      "source": [
        "plots(test_df, 'NO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwscg-4d-YSN"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('test_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ijYCYWFrOjq"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHpB0v6O8b49"
      },
      "source": [
        "\n",
        "## Custom Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mmRRWvcz6Z6"
      },
      "outputs": [],
      "source": [
        "def get_random_vectors(batch_size=1, latent_dim=64):\n",
        "    \"\"\"Noise generator\"\"\"\n",
        "    return tf.random.normal(shape=(batch_size, latent_dim))\n",
        "\n",
        "\n",
        "seed = get_random_vectors(\n",
        "    PARAMS.get('BATCH_SIZE'),\n",
        "    PARAMS.get('LATENT_DIM')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1czpg8y0xJ4"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwzHhr3xG7vq"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(input_shape=(32, 32, 1), latent_dim=128):\n",
        "    \"\"\"\n",
        "    Returns GAN discriminator model\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential(name='discriminator')\n",
        "\n",
        "    # input layer\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "    # conv layers\n",
        "    conv_layers = [\n",
        "        layers.Conv2D(1, 8, strides=1, padding=\"same\"),\n",
        "        layers.Conv2D(latent_dim//8, 8, strides=2, padding=\"same\"),\n",
        "        layers.Conv2D(latent_dim//4, 4, strides=2, padding=\"same\"),\n",
        "        layers.Conv2D(latent_dim//2, 2, strides=2, padding=\"same\"),\n",
        "        layers.Conv2D(latent_dim, 2, strides=2, padding=\"same\"),\n",
        "    ]\n",
        "    for c in conv_layers:\n",
        "        model.add(c)\n",
        "        model.add(layers.BatchNormalization()),\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "    # dense layers\n",
        "    dense_lays = [\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(latent_dim),\n",
        "        layers.Dropout(0.2)\n",
        "    ]\n",
        "    for d in dense_lays:\n",
        "        model.add(d)\n",
        "\n",
        "    # output layer\n",
        "    model.add(layers.Dense(1))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O2gOh--t35k"
      },
      "outputs": [],
      "source": [
        "discriminator = build_discriminator(PARAMS.get('INPUT_SHAPE'))\n",
        "my_plot_model(discriminator)\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl-ovZkzrZo9"
      },
      "outputs": [],
      "source": [
        "def build_generator(img_size=32, latent_dim=128):\n",
        "    \"\"\"\n",
        "    Returns GAN generator model\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential(name='generator')\n",
        "\n",
        "    # input layer\n",
        "    model.add(layers.InputLayer(input_shape=(latent_dim,)))\n",
        "\n",
        "    # dense layer\n",
        "    shaped_size = img_size // 32\n",
        "    model.add(layers.Dense(shaped_size**2 * latent_dim))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Reshape((shaped_size, shaped_size, latent_dim)))\n",
        "\n",
        "    # 5 convt layers\n",
        "    for c in range(5):\n",
        "        model.add(\n",
        "            layers.Conv2DTranspose(\n",
        "                latent_dim, 2,\n",
        "                strides=2,\n",
        "                padding=\"same\"\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.LeakyReLU(alpha=0.2)),\n",
        "\n",
        "    # output layer\n",
        "    model.add(\n",
        "        layers.Conv2DTranspose(\n",
        "            1, 2,\n",
        "            strides=1,\n",
        "            padding=\"same\",\n",
        "            activation='tanh'\n",
        "        )\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45q0KPxzt5S7"
      },
      "outputs": [],
      "source": [
        "generator = build_generator(\n",
        "    PARAMS['INPUT_SHAPE'][1],\n",
        "    PARAMS.get('LATENT_DIM')\n",
        ")\n",
        "my_plot_model(generator)\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkfAM5YwQt4N"
      },
      "source": [
        "### Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHJhWknVrUyk"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Custom GAN class model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator,\n",
        "        discriminator,\n",
        "        params: dict,\n",
        "        name='GAN',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.latent_dim = params.get('LATENT_DIM')\n",
        "        self.seed = seed\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        super().compile()\n",
        "\n",
        "    def call(self, inputs) -> List[tf.Tensor]:\n",
        "        fakes = self.generator(\n",
        "            get_random_vectors(\n",
        "                tf.shape(inputs)[0], self.latent_dim\n",
        "            )\n",
        "        )\n",
        "        predictions = [\n",
        "            self.discriminator(inputs),\n",
        "            self.discriminator(fakes)\n",
        "        ]\n",
        "        return [fakes, predictions]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = get_random_vectors(batch_size, self.latent_dim)\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat(\n",
        "            [generated_images, real_images],\n",
        "            axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n",
        "            axis=0\n",
        "        )\n",
        "\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "            grads = tape.gradient(\n",
        "                d_loss,\n",
        "                self.discriminator.trainable_variables\n",
        "            )\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(grads, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        # Sample random points in the latent space (again)\n",
        "        random_latent_vectors = get_random_vectors(batch_size, self.latent_dim)\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator\n",
        "        # (note that we should *not* update the weights of the discriminator)\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_images = self.generator(random_latent_vectors)\n",
        "            predictions = self.discriminator(generated_images)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "            grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "            self.g_optimizer.apply_gradients(\n",
        "                zip(grads, self.generator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoZAcD1xycmH"
      },
      "outputs": [],
      "source": [
        "# Best - 1e4, 3e4\n",
        "\n",
        "DIS_LR = 0.0001   # @param {type:\"number\"}\n",
        "d_optimizer = tf.keras.optimizers.Adam(learning_rate=DIS_LR)\n",
        "\n",
        "GEN_LR = 0.0003   # @param {type:\"number\"}\n",
        "g_optimizer = tf.keras.optimizers.Adam(learning_rate=GEN_LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZUljcmb4XFA"
      },
      "outputs": [],
      "source": [
        "# Build GAN\n",
        "gan = GAN(\n",
        "    generator,\n",
        "    discriminator,\n",
        "    PARAMS,\n",
        ")\n",
        "gan.compile(\n",
        "    d_optimizer=d_optimizer,\n",
        "    g_optimizer=g_optimizer,\n",
        "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1DR0KGdzXSn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msLQzAU1RVgH"
      },
      "source": [
        "\n",
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0jH_AiFWcY1"
      },
      "outputs": [],
      "source": [
        "GAN_CKPT_DIR = './gan_ckpts'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFDXOuQXwMKV"
      },
      "outputs": [],
      "source": [
        "class myGANCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Show and save imgs per epoch (checkpoint)\n",
        "    \"\"\"\n",
        "    def __init__(self, dir: str, manager):\n",
        "        self.dir = dir\n",
        "        self.manager = manager\n",
        "        self.loss_lim = 1.\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs={}):\n",
        "        \"\"\"\n",
        "        chkpt and imgs save\n",
        "        \"\"\"\n",
        "        result = self.model.generator(self.model.seed)\n",
        "        show_img(result)\n",
        "        # display.clear_output(wait=True)\n",
        "        \n",
        "        # FFS?\n",
        "        g_loss = logs.get('g_loss')\n",
        "        d_loss = logs.get('d_loss')\n",
        "        \n",
        "        #if g_loss < d_loss:\n",
        "        if abs(g_loss - d_loss) < self.loss_lim:\n",
        "            # ckpt\n",
        "            self.manager.save()\n",
        "            save_img(result, epoch, self.dir)\n",
        "            # loss lim rate\n",
        "            self.loss_lim *= 0.9\n",
        "        else:\n",
        "            if (epoch % 10) == 0:\n",
        "                save_img(result, epoch, self.dir)\n",
        "\n",
        "        # TODO: Dynamic Learn, FFS?\n",
        "        if False:\n",
        "            optz = [\n",
        "                self.model.g_optimizer,\n",
        "                self.model.d_optimizer,\n",
        "            ]\n",
        "            for o in optz:\n",
        "                old_lr = o.lr.read_value()\n",
        "                new_lr = old_lr * 0.99\n",
        "                o.lr.assign(new_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p02xI8nTOq3"
      },
      "outputs": [],
      "source": [
        "def gan_ckpt():\n",
        "    \"\"\"\n",
        "    GAN Checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = tf.train.Checkpoint(\n",
        "        g_optimizer=g_optimizer,\n",
        "        d_optimizer=d_optimizer,\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "    )\n",
        "    manager = tf.train.CheckpointManager(\n",
        "        checkpoint,\n",
        "        directory=GAN_CKPT_DIR,\n",
        "        max_to_keep=5\n",
        "    )\n",
        "    checkpoint.restore(manager.latest_checkpoint)\n",
        "    return manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYxqPPTlsj8I"
      },
      "outputs": [],
      "source": [
        "def gan_callbacks(dir: str) -> list:\n",
        "    \"\"\"\n",
        "    Set GAN training callbacks\n",
        "    \"\"\"\n",
        "    return [\n",
        "        myGANCallback(dir, gan_ckpt()),\n",
        "        tensorboard_callback\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FXt7CqaVZOR"
      },
      "source": [
        "### Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMRA9cyt0bc6"
      },
      "outputs": [],
      "source": [
        "def prep_fn(img):\n",
        "    \"\"\"\n",
        "    Normalize image preprocess function [-1, 1]\n",
        "    \"\"\"\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = (img - 0.5) * 2\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_gan_gen(df: DataFrame):\n",
        "    \"\"\"\n",
        "    Returns GANs input flow\n",
        "    \"\"\"\n",
        "    datagen_gan = ImageDataGenerator(\n",
        "        preprocessing_function=prep_fn,\n",
        "        #rotation_range=90,\n",
        "        #horizontal_flip=True,\n",
        "        #vertical_flip=True,\n",
        "    )\n",
        "    return datagen_gan.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=DATASET_PATH,\n",
        "        x_col='path', y_col='lbl',\n",
        "        batch_size=PARAMS['BATCH_SIZE'],\n",
        "        seed=42,\n",
        "        shuffle=True,\n",
        "        color_mode='grayscale',\n",
        "        classes=['YES'],\n",
        "        target_size=(\n",
        "            PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE']\n",
        "        ),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17aDD1NSSVbG"
      },
      "outputs": [],
      "source": [
        "def gan_kwargs(name: str, eps: int, df_name='relearn_df.csv') -> dict:\n",
        "    \"\"\"\n",
        "    Custom GAN fit\n",
        "    \"\"\"\n",
        "    dir = f'GAN_train/{name}'\n",
        "    try:\n",
        "        os.mkdir(dir)\n",
        "    except FileExistsError as e:\n",
        "        exit(f'{e}: {dir} already exists')\n",
        "\n",
        "    gan_df = pd.read_csv(df_name)\n",
        "    return {\n",
        "        'x': get_gan_gen(gan_df),\n",
        "        'epochs': eps,\n",
        "        'callbacks': gan_callbacks(dir),\n",
        "        'verbose': 1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiihrsYagAKa"
      },
      "source": [
        "### Restore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKdLot8eddIk"
      },
      "outputs": [],
      "source": [
        "os.mkdir('GAN_train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRMqLdmAKkKd"
      },
      "outputs": [],
      "source": [
        "unzip_files(['gan_ckpts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFlnfL7kRYtz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl02c8cSSqin"
      },
      "source": [
        "### Single"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS682aduzEc_"
      },
      "outputs": [],
      "source": [
        "FIT_NAME = \"test\"  # @param {type:\"string\"}\n",
        "gan_eps = 10        # @param {type:\"integer\"}\n",
        "\n",
        "train_kwargs = gan_kwargs(FIT_NAME, gan_eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWAGbrUQXSqP"
      },
      "outputs": [],
      "source": [
        "gan_history = fit_model(gan, train_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N4mS4eHSsxV"
      },
      "source": [
        "### Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpm3DkAtvx9x"
      },
      "outputs": [],
      "source": [
        "@timed\n",
        "def train_loop(gan_model, version: int, loops:int=10) -> List:\n",
        "    history = []\n",
        "    for i in range(loops):\n",
        "        FIT_NAME = f\"train_{version}_{i}\"\n",
        "        # Setup train (100 epochs)\n",
        "        gan_eps = 100\n",
        "        train_kwargs = gan_kwargs(FIT_NAME, gan_eps)\n",
        "\n",
        "        # Custom fit\n",
        "        gan_history = fit_model(gan_model, train_kwargs)\n",
        "        history.append(gan_history)\n",
        "\n",
        "        # Save\n",
        "        zip_files(['GAN_train', 'gan_ckpts'])\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lypR9gsn6DjB"
      },
      "outputs": [],
      "source": [
        "version =          1# @param {type:\"integer\"}\n",
        "gan_history = train_loop(\n",
        "    gan, version,\n",
        "    loops=1         # @param {type:\"integer\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmoOXXe-Vn9m"
      },
      "outputs": [],
      "source": [
        "plt.plot(gan_history[0].history['g_loss'])\n",
        "plt.plot(gan_history[0].history['d_loss'])\n",
        "plt.title('GAN train loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(\n",
        "    ['generator', 'discriminator'],\n",
        "    loc='upper left'\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn6V3U5H8_ZQ"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnICXU4X7Q7k"
      },
      "outputs": [],
      "source": [
        "GAN_NAME = 'gan'    # @param {type: 'string'}\n",
        "gan.save(GAN_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00fEgonqhYKm"
      },
      "outputs": [],
      "source": [
        "zip_files([GAN_NAME])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdhH3wPth-77"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoYOr6ny0XDy"
      },
      "source": [
        "## Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGQR-WFmVdZB"
      },
      "source": [
        "### Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ1FI-Rb_Lgo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "\n",
        "\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "\n",
        "@timed\n",
        "def make_gans_batch(model, name: str, samples=10):\n",
        "    \"\"\"\n",
        "    Build local batch from trained generator\n",
        "    \"\"\"\n",
        "    p_name = f'GAN_samples/{name}'\n",
        "    try:\n",
        "        os.mkdir(p_name)\n",
        "    except FileExistsError as e:\n",
        "        print(e)\n",
        "\n",
        "    data = {\n",
        "        'id': [],\n",
        "        'path': [],\n",
        "    }\n",
        "    for i in range(samples):\n",
        "        result = model(\n",
        "            get_random_vectors(\n",
        "                PARAMS.get('BATCH_SIZE'),\n",
        "                PARAMS.get('LATENT_DIM')\n",
        "            )\n",
        "        )\n",
        "        # TODO: control FFS\n",
        "        path = save_img(result, i, p_name)\n",
        "        data['path'].append(f'/content/{path}')\n",
        "\n",
        "        id = path.split('/')[-1]\n",
        "        data['id'].append(id)\n",
        "        show_img(result)\n",
        "\n",
        "    samples_df = DataFrame(data)\n",
        "    samples_df['lbl'] = ['YES' for i in range(samples)]\n",
        "\n",
        "    f_name = f'samples_{name}_df.csv'\n",
        "    samples_df.to_csv(f'{p_name}/{f_name}', index=False)\n",
        "    return samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe1jaDaQBmz8"
      },
      "outputs": [],
      "source": [
        "os.mkdir('GAN_samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Ia6r2eVrnb"
      },
      "source": [
        "### Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9Q_HvHfsHMGH"
      },
      "outputs": [],
      "source": [
        "BATCH_NAME = 'results'   # @param {type: 'string'}\n",
        "SAMPLES_SIZE = 2000     # @param {type: 'integer'}\n",
        "\n",
        "samples_df = make_gans_batch(\n",
        "    gan.generator,\n",
        "    BATCH_NAME,\n",
        "    SAMPLES_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1w_JULaAV_7v"
      },
      "outputs": [],
      "source": [
        "samples_df = caract_df(\n",
        "    f'/content/GAN_samples/{BATCH_NAME}/samples_{BATCH_NAME}_df.csv',\n",
        "    f'/content/GAN_samples/{BATCH_NAME}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqRYTBzDEjgc"
      },
      "outputs": [],
      "source": [
        "def plot_avg(results: List):\n",
        "    fig = plt.figure(constrained_layout=True)\n",
        "\n",
        "    ax = fig.add_subplot()\n",
        "    ax.plot(results[:4, :].T)\n",
        "    ax.set_xlabel(\"Distance\")\n",
        "    ax.set_ylabel(\"Value\")\n",
        "    ax.set_title(\"Avg Correlations\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_separate(r: List):\n",
        "    fig, (ax1, ax2) = plt.subplots(\n",
        "        1, 2,\n",
        "        sharex=True, sharey=True,\n",
        "        constrained_layout=True\n",
        "    )\n",
        "    \n",
        "    ax1.set_xlabel(\"Distance\")\n",
        "    ax1.set_ylabel(\"Value\")\n",
        "    ax1.set_title('1s')\n",
        "    ax1.plot(r[0, :].T, 'tab:blue', label='F2P')\n",
        "    ax1.plot(r[1, :].T, 'tab:orange', label='FLP')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_xlabel(\"Distance\")\n",
        "    ax2.set_ylabel(\"Value\")\n",
        "    ax2.set_title('0s')\n",
        "    ax2.plot(r[2, :].T, 'tab:green', label='F2P')\n",
        "    ax2.plot(r[3, :].T, 'tab:red', label='FLP')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "@timed\n",
        "def plots(df: DataFrame, lbl: str):\n",
        "    \"\"\"\n",
        "    Plots results\n",
        "    \"\"\"\n",
        "    r = df['ff'][df['lbl'] == lbl].mean()\n",
        "    \n",
        "    plot_avg(r)\n",
        "    plot_separate(r)\n",
        "   \n",
        "    #plt.savefig(f'{path}/plot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFmhx6HUwLvN"
      },
      "outputs": [],
      "source": [
        "df = test_df\n",
        "title = 'Test samples avg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6JwFJcg9x9wA"
      },
      "outputs": [],
      "source": [
        "df = samples_df\n",
        "title = 'GANs samples avg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g5EJAzlns1jI"
      },
      "outputs": [],
      "source": [
        "r = df['ff'][df['lbl'] == \"YES\"].mean()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(\n",
        "    2, 1,\n",
        "    sharex=True, sharey=False,\n",
        "    constrained_layout=True\n",
        ")\n",
        "ax1.set_xlabel(\"Distance\")\n",
        "ax1.set_ylabel(\"Value\")\n",
        "ax1.set_title('F2P')\n",
        "ax1.plot(r[0, :].T, 'tab:blue', label='1s')\n",
        "ax1.plot(r[2, :].T, 'tab:orange', label='0s')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.set_xlabel(\"Distance\")\n",
        "ax2.set_ylabel(\"Value\")\n",
        "ax2.set_title('FLP')\n",
        "ax2.plot(r[1, :].T, 'tab:green', label='1s')\n",
        "ax2.plot(r[3, :].T, 'tab:red', label='0s')\n",
        "ax2.legend()\n",
        "\n",
        "fig.suptitle(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21i5qlG-VYYB"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpMb5EPjtzZY"
      },
      "outputs": [],
      "source": [
        "#zip_files(['GAN_samples'])\n",
        "zip_files(['gan_ckpts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l46APZW1iTDz"
      },
      "source": [
        "## Restore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bakASm0yvF_X"
      },
      "source": [
        "### Unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Le4NKg_hxaL"
      },
      "outputs": [],
      "source": [
        "unzip_files(['gan'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eza7GufPsIjI"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "unzip_name = 'gan'     # @param {type: 'string'}\n",
        "\n",
        "#saved_gan = tf.saved_model.load(\n",
        "saved_gan = tf.keras.models.load_model(\n",
        "    f'/content/{unzip_name}',\n",
        "    #custom_objects={\"GAN\": GAN}\n",
        ")\n",
        "saved_gan.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ovZYYF_vXcU"
      },
      "source": [
        "### FFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoqmpTu8iobR"
      },
      "outputs": [],
      "source": [
        "# SOURCE\n",
        "sample_generator = DATAGEN.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=DATASET_PATH,\n",
        "    x_col=\"path\", y_col=\"lbl\",\n",
        "    batch_size=1,\n",
        "    colormode='grayscale',\n",
        "    shuffle=True,\n",
        "    classes=['YES'],\n",
        "    target_size=(\n",
        "        PARAMS['IMG_SIZE'], PARAMS['IMG_SIZE'],\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifoT3LONi-d_"
      },
      "outputs": [],
      "source": [
        "img, _ = next(sample_generator)\n",
        "show_img(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PvAV9v9jtKy"
      },
      "outputs": [],
      "source": [
        "# FFS\n",
        "true_ffs = caracterization(img, PARAMS['IMG_SIZE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iShLCiNeywOv"
      },
      "outputs": [],
      "source": [
        "title = 'Test'\n",
        "results = true_ffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upW15zIgygjy"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(constrained_layout=True)\n",
        "\n",
        "ax = fig.add_subplot()\n",
        "ax.plot(results[:4, :].T)\n",
        "ax.set_xlabel(\"Distance\")\n",
        "ax.set_ylabel(\"Value\")\n",
        "ax.set_title(f\"Avg Correlations {title} sample\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S4YvVLHO_EJc"
      },
      "outputs": [],
      "source": [
        "# SAMPLE\n",
        "PARAMS['LATENT_DIM'] = 64\n",
        "#img = saved_gan.generator(\n",
        "img = gan.generator(\n",
        "    get_random_vectors(\n",
        "        PARAMS.get('BATCH_SIZE'),\n",
        "        PARAMS.get('LATENT_DIM')\n",
        "    )\n",
        ")\n",
        "show_img(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CTxJmnhMvrqS"
      },
      "outputs": [],
      "source": [
        "# FFS\n",
        "false_ffs = caracterization(img.numpy(), PARAMS.get('IMG_SIZE'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPUNjIXwUUE0"
      },
      "outputs": [],
      "source": [
        "r = true_ffs\n",
        "\n",
        "fig = plt.figure(constrained_layout=True)\n",
        "\n",
        "ax = fig.add_subplot()\n",
        "ax.plot(r[0, :].T, 'tab:blue', label='F2P 1s')\n",
        "ax.plot(r[1, :].T, 'tab:green', label='F2P 1s')\n",
        "ax.plot(r[2, :].T, 'tab:orange', label='FLP 0s')\n",
        "ax.plot(r[3, :].T, 'tab:red', label='FLP 0s')\n",
        "\n",
        "ax.set_xlabel(\"Distance\")\n",
        "ax.set_ylabel(\"Value\")\n",
        "ax.set_title(\"Single img correlations\")\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hTtTGGEvQrK"
      },
      "source": [
        "\n",
        "### Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P4O6YkSgdEm"
      },
      "outputs": [],
      "source": [
        "# Batch\n",
        "name = 'saved'    # @param {type: 'string'}\n",
        "\n",
        "saved_samples_df = make_gans_batch(\n",
        "    #saved_gan.generator, name, 5)\n",
        "    gan.generator,\n",
        "    name, 100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esJx2NWQUhTc"
      },
      "outputs": [],
      "source": [
        "# Caract DF\n",
        "saved_samples_df = caract_df(\n",
        "    f'samples_{name}_df.csv',\n",
        "    f'/content/GAN_samples/{name}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2_IzBE0zu_n"
      },
      "outputs": [],
      "source": [
        "plots(saved_samples_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qbQnbPsk98en",
        "hb38hOYQ5vVK",
        "OuT66h6uLzgb",
        "pddSaBAShTJE",
        "Z0731yIxBi3n",
        "yY7jmNnYe-eZ",
        "vVhui3nEa9Ch",
        "uukvHrXWojT0",
        "jLzjbYXlXMaf",
        "Zwe-hfCLOBFs",
        "IyYAV0KjbgOr",
        "z1czpg8y0xJ4",
        "DkfAM5YwQt4N",
        "msLQzAU1RVgH",
        "4FXt7CqaVZOR",
        "Zl02c8cSSqin"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}